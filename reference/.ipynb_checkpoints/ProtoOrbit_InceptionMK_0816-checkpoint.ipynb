{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjeJDijzIRKy",
    "outputId": "d3abb754-a882-45aa-afdb-ce14c18a8974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Train: (7352, 128, 9)  Test: (2947, 128, 9)\n",
      "[Device] Using: cuda\n",
      "[Epoch 01] loss=0.7486 (act=0.4961, rot=0.5481, orbit=0.8802) | test acc=85.71% f1=0.8481\n",
      "  ★ New best F1: 0.8481\n",
      "[Epoch 02] loss=0.3765 (act=0.2559, rot=0.2286, orbit=0.5202) | test acc=90.36% f1=0.9037\n",
      "  ★ New best F1: 0.9037\n",
      "[Epoch 03] loss=0.3412 (act=0.2277, rot=0.2248, orbit=0.4607) | test acc=91.01% f1=0.9091\n",
      "  ★ New best F1: 0.9091\n",
      "[Epoch 04] loss=0.3240 (act=0.2218, rot=0.1963, orbit=0.4322) | test acc=88.70% f1=0.8859\n",
      "[Epoch 05] loss=0.2900 (act=0.1986, rot=0.1676, orbit=0.4106) | test acc=92.03% f1=0.9201\n",
      "  ★ New best F1: 0.9201\n",
      "[Epoch 06] loss=0.2804 (act=0.1948, rot=0.1520, orbit=0.4000) | test acc=91.21% f1=0.9120\n",
      "[Epoch 07] loss=0.2631 (act=0.1816, rot=0.1440, orbit=0.3831) | test acc=89.89% f1=0.8976\n",
      "[Epoch 08] loss=0.2682 (act=0.1855, rot=0.1482, orbit=0.3830) | test acc=88.84% f1=0.8880\n",
      "[Epoch 09] loss=0.2513 (act=0.1716, rot=0.1426, orbit=0.3688) | test acc=92.09% f1=0.9211\n",
      "  ★ New best F1: 0.9211\n",
      "[Epoch 10] loss=0.2511 (act=0.1762, rot=0.1272, orbit=0.3679) | test acc=91.75% f1=0.9179\n",
      "[Epoch 11] loss=0.2404 (act=0.1664, rot=0.1266, orbit=0.3601) | test acc=91.41% f1=0.9132\n",
      "[Epoch 12] loss=0.2396 (act=0.1657, rot=0.1264, orbit=0.3598) | test acc=90.97% f1=0.9098\n",
      "[Epoch 13] loss=0.2326 (act=0.1630, rot=0.1151, orbit=0.3512) | test acc=91.72% f1=0.9172\n",
      "[Epoch 14] loss=0.2228 (act=0.1561, rot=0.1076, orbit=0.3447) | test acc=92.67% f1=0.9270\n",
      "  ★ New best F1: 0.9270\n",
      "[Epoch 15] loss=0.2134 (act=0.1503, rot=0.0983, orbit=0.3361) | test acc=92.06% f1=0.9200\n",
      "[Epoch 16] loss=0.2150 (act=0.1544, rot=0.0903, orbit=0.3350) | test acc=91.99% f1=0.9199\n",
      "[Epoch 17] loss=0.2075 (act=0.1477, rot=0.0899, orbit=0.3287) | test acc=92.37% f1=0.9238\n",
      "[Epoch 18] loss=0.2068 (act=0.1476, rot=0.0878, orbit=0.3285) | test acc=93.15% f1=0.9317\n",
      "  ★ New best F1: 0.9317\n",
      "[Epoch 19] loss=0.1950 (act=0.1394, rot=0.0778, orbit=0.3229) | test acc=92.23% f1=0.9229\n",
      "[Epoch 20] loss=0.1979 (act=0.1425, rot=0.0777, orbit=0.3208) | test acc=92.13% f1=0.9212\n",
      "[Epoch 21] loss=0.1926 (act=0.1400, rot=0.0682, orbit=0.3212) | test acc=91.99% f1=0.9194\n",
      "[Epoch 22] loss=0.1925 (act=0.1375, rot=0.0769, orbit=0.3190) | test acc=92.87% f1=0.9283\n",
      "[Epoch 23] loss=0.2005 (act=0.1409, rot=0.0923, orbit=0.3192) | test acc=92.23% f1=0.9219\n",
      "[Epoch 24] loss=0.1832 (act=0.1321, rot=0.0656, orbit=0.3135) | test acc=92.13% f1=0.9212\n",
      "[Epoch 25] loss=0.1854 (act=0.1336, rot=0.0665, orbit=0.3182) | test acc=92.98% f1=0.9300\n",
      "\n",
      "[Final - No TTA] ACC=93.15%  F1=0.9317\n",
      "TTA evaluation failed: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "\n",
      "=== Training Complete ===\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UCI-HAR (Inertial Signals, raw 9ch) Training & Evaluation\n",
    "# InceptionMK + Rotation SSL + (optional) Rotation-Consistency + RoCo-TTA\n",
    "# + Proto-Orbit Contrast (class-prototype-based rotation-invariant loss)\n",
    "# FIXED VERSION - Resolving gradient flow issues\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Utilities: download & load UCI-HAR (raw inertial)\n",
    "# ----------------------------\n",
    "UCI_URL = \"https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\"\n",
    "\n",
    "def download_and_extract_uci(root: str):\n",
    "    root = Path(root)\n",
    "    data_dir = root / \"UCI_HAR_Dataset\"\n",
    "    if data_dir.exists():\n",
    "        return data_dir\n",
    "\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = root / \"uci_har.zip\"\n",
    "    print(\"[UCI] Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(UCI_URL, zip_path)\n",
    "\n",
    "    print(\"[UCI] Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(root)\n",
    "\n",
    "    # The archive extracts a folder \"UCI HAR Dataset\"\n",
    "    # unify name to UCI_HAR_Dataset\n",
    "    original = root / \"UCI HAR Dataset\"\n",
    "    if original.exists():\n",
    "        original.rename(data_dir)\n",
    "    zip_path.unlink(missing_ok=True)\n",
    "    print(\"[UCI] Ready at:\", str(data_dir))\n",
    "    return data_dir\n",
    "\n",
    "def _load_signal_file(path: Path) -> np.ndarray:\n",
    "    # Each file: N lines, each line has 128 floats separated by space\n",
    "    arr = np.loadtxt(path, dtype=np.float32)\n",
    "    # shape: (N, 128)\n",
    "    return arr\n",
    "\n",
    "def load_uci_inertial(data_dir: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_train: (N_train, 128, 9), y_train: (N_train,)\n",
    "      X_test:  (N_test, 128, 9), y_test:  (N_test,)\n",
    "      label_names: list of 6 activities\n",
    "    \"\"\"\n",
    "    base = data_dir\n",
    "\n",
    "    # 9 inertial signals\n",
    "    train_inertial = base / \"train\" / \"Inertial Signals\"\n",
    "    test_inertial  = base / \"test\" / \"Inertial Signals\"\n",
    "\n",
    "    files = [\n",
    "        \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
    "        \"body_acc_x\",  \"body_acc_y\",  \"body_acc_z\",\n",
    "        \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\"\n",
    "    ]\n",
    "    # Build arrays by stacking channel-wise\n",
    "    Xtr_list = []\n",
    "    Xte_list = []\n",
    "    for name in files:\n",
    "        ftr = train_inertial / f\"{name}_train.txt\"\n",
    "        fte = test_inertial  / f\"{name}_test.txt\"\n",
    "        Xtr_list.append(_load_signal_file(ftr))  # (Ntr,128)\n",
    "        Xte_list.append(_load_signal_file(fte))  # (Nte,128)\n",
    "\n",
    "    # Stack -> (C, N, 128) then transpose\n",
    "    X_train = np.stack(Xtr_list, axis=2)  # (Ntr,128,9)\n",
    "    X_test  = np.stack(Xte_list, axis=2)  # (Nte,128,9)\n",
    "\n",
    "    # Labels: 1..6\n",
    "    y_train = np.loadtxt(base / \"train\" / \"y_train.txt\", dtype=np.int64).ravel() - 1\n",
    "    y_test  = np.loadtxt(base / \"test\" / \"y_test.txt\", dtype=np.int64).ravel() - 1\n",
    "\n",
    "    # Label names\n",
    "    label_names = [\"WALKING\",\"WALKING_UPSTAIRS\",\"WALKING_DOWNSTAIRS\",\"SITTING\",\"STANDING\",\"LAYING\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, label_names\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Rotation utilities (3-axis group-wise)\n",
    "# ----------------------------\n",
    "def _batched_rotation_matrix(angles):  # angles: (B, 3) in radians: rx, ry, rz\n",
    "    B = angles.shape[0]\n",
    "    rx, ry, rz = angles[:, 0], angles[:, 1], angles[:, 2]\n",
    "\n",
    "    cx, sx = torch.cos(rx), torch.sin(rx)\n",
    "    cy, sy = torch.cos(ry), torch.sin(ry)\n",
    "    cz, sz = torch.cos(rz), torch.sin(rz)\n",
    "\n",
    "    Rx = torch.zeros(B, 3, 3, device=angles.device, dtype=angles.dtype)\n",
    "    Rx[:, 0, 0] = 1.\n",
    "    Rx[:, 1, 1] = cx; Rx[:, 1, 2] = -sx\n",
    "    Rx[:, 2, 1] = sx; Rx[:, 2, 2] = cx\n",
    "\n",
    "    Ry = torch.zeros(B, 3, 3, device=angles.device, dtype=angles.dtype)\n",
    "    Ry[:, 0, 0] = cy;  Ry[:, 0, 2] = sy\n",
    "    Ry[:, 1, 1] = 1.\n",
    "    Ry[:, 2, 0] = -sy; Ry[:, 2, 2] = cy\n",
    "\n",
    "    Rz = torch.zeros(B, 3, 3, device=angles.device, dtype=angles.dtype)\n",
    "    Rz[:, 0, 0] = cz; Rz[:, 0, 1] = -sz\n",
    "    Rz[:, 1, 0] = sz; Rz[:, 1, 1] = cz\n",
    "    Rz[:, 2, 2] = 1.\n",
    "\n",
    "    R = Rz @ Ry @ Rx\n",
    "    return R  # (B, 3, 3)\n",
    "\n",
    "def apply_random_rotation_xyz(x, max_deg=20.0, group_size=3):\n",
    "    \"\"\"\n",
    "    x: (B, T, C)\n",
    "    C must be multiple of 3. UCI inertial: 9ch -> OK.\n",
    "    \"\"\"\n",
    "    B, T, C = x.shape\n",
    "    assert C % group_size == 0\n",
    "    G = C // group_size\n",
    "\n",
    "    max_rad = max_deg * math.pi / 180.0\n",
    "    angles = (torch.rand(B, 3, device=x.device, dtype=x.dtype) * 2 - 1) * max_rad\n",
    "    R = _batched_rotation_matrix(angles)  # (B,3,3)\n",
    "\n",
    "    x_rot = x.clone()\n",
    "    for g in range(G):\n",
    "        seg = x[:, :, g*group_size:(g+1)*group_size]      # (B, T, 3)\n",
    "        seg2 = seg.reshape(B*T, 3)\n",
    "        Rb = R.repeat_interleave(T, dim=0)                # (B*T,3,3)\n",
    "        seg_rot = (Rb @ seg2.unsqueeze(-1)).squeeze(-1)   # (B*T,3)\n",
    "        x_rot[:, :, g*group_size:(g+1)*group_size] = seg_rot.view(B, T, 3)\n",
    "    return x_rot\n",
    "\n",
    "def make_rotation_class(x, num_rotations=4):\n",
    "    \"\"\"Fixed version with proper tensor handling\"\"\"\n",
    "    device = x.device\n",
    "    B, T, C = x.shape\n",
    "    angles_deg = torch.tensor([0., 90., 180., 270.], device=device, dtype=x.dtype)\n",
    "    labels = torch.randint(0, len(angles_deg), (B,), device=device, dtype=torch.long)\n",
    "    rad = angles_deg[labels] * math.pi / 180.0\n",
    "\n",
    "    angles = torch.stack([torch.zeros_like(rad), torch.zeros_like(rad), rad], dim=1)  # (B,3)\n",
    "    R = _batched_rotation_matrix(angles)  # (B,3,3)\n",
    "\n",
    "    x_rot = x.clone()\n",
    "    G = C // 3\n",
    "    for g in range(G):\n",
    "        seg = x[:, :, g*3:(g+1)*3]           # (B,T,3)\n",
    "        seg2 = seg.reshape(B*T, 3)\n",
    "        Rb = R.repeat_interleave(T, dim=0)   # (B*T,3,3)\n",
    "        seg_rot = (Rb @ seg2.unsqueeze(-1)).squeeze(-1)\n",
    "        x_rot[:, :, g*3:(g+1)*3] = seg_rot.view(B, T, 3)\n",
    "\n",
    "    return x_rot, labels\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model (InceptionMK + adapter + TTA + Prototypes)\n",
    "# ----------------------------\n",
    "class DSConv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                   padding=kernel_size // 2, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "        self.branch2 = DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3 = DSConv1D(in_channels, out_channels, kernel_size=3)\n",
    "        self.branch4 = DSConv1D(in_channels, out_channels, kernel_size=5)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([\n",
    "            self.branch1(x), self.branch2(x),\n",
    "            self.branch3(x), self.branch4(x)\n",
    "        ], dim=1)\n",
    "        return self.relu(out)\n",
    "\n",
    "class MultiKernelBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = DSConv1D(in_channels, out_channels, kernel_size=3)\n",
    "        self.branch3 = DSConv1D(in_channels, out_channels, kernel_size=5)\n",
    "        self.branch4 = DSConv1D(in_channels, out_channels, kernel_size=7)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([\n",
    "            self.branch1(x), self.branch2(x),\n",
    "            self.branch3(x), self.branch4(x)\n",
    "        ], dim=1)\n",
    "        return self.relu(out)\n",
    "\n",
    "class FeatureAdapter(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, dim))\n",
    "        self.beta  = nn.Parameter(torch.zeros(1, dim))\n",
    "    def forward(self, z):\n",
    "        return z * self.gamma + self.beta\n",
    "\n",
    "class InceptionMK(nn.Module):\n",
    "    def __init__(self, input_channels=9, stem_out=64, block_out=32,\n",
    "                 embedding_dim=128, num_classes=6, num_rotations=4):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(stem_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.inception = InceptionBlock(stem_out, block_out)\n",
    "        self.mk_block  = MultiKernelBlock(block_out * 4, block_out)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc  = nn.Linear(block_out * 4, embedding_dim)\n",
    "        self.adapter = FeatureAdapter(embedding_dim)\n",
    "\n",
    "        self.head_act = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.head_rot = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_rotations)\n",
    "        )\n",
    "\n",
    "        # Proto-Orbit: class prototypes (EMA)\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.register_buffer(\"prototypes\", torch.zeros(num_classes, embedding_dim))\n",
    "        self.register_buffer(\"proto_counts\", torch.zeros(num_classes))\n",
    "        self.proto_m = 0.99\n",
    "\n",
    "    def forward(self, x):  # x: (B,T,C)\n",
    "        x = x.transpose(1, 2)           # -> (B,C,T)\n",
    "        x = self.stem(x)\n",
    "        x = self.inception(x)\n",
    "        x = self.mk_block(x)\n",
    "        x = self.gap(x).squeeze(-1)     # (B, 4*block_out)\n",
    "        z = self.fc(x)                  # (B, D)\n",
    "        z = self.adapter(z)\n",
    "        logits_act = self.head_act(z)\n",
    "        logits_rot = self.head_rot(z)\n",
    "        return logits_act, logits_rot, z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_prototypes(self, z, y):\n",
    "        \"\"\"Fixed prototype update with proper initialization\"\"\"\n",
    "        if z.size(0) == 0:\n",
    "            return\n",
    "\n",
    "        z_n = F.normalize(z, dim=-1)\n",
    "        unique_classes = y.unique()\n",
    "\n",
    "        for c in unique_classes:\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                z_class = z_n[mask].mean(dim=0)\n",
    "                count = self.proto_counts[c]\n",
    "\n",
    "                if count == 0:\n",
    "                    # First update: direct assignment\n",
    "                    self.prototypes[c] = z_class\n",
    "                    self.proto_counts[c] = 1\n",
    "                else:\n",
    "                    # EMA update\n",
    "                    self.prototypes[c] = self.proto_m * self.prototypes[c] + (1 - self.proto_m) * z_class\n",
    "                    self.proto_counts[c] += 1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2.5) Rotation-consistency (optional)\n",
    "# ----------------------------\n",
    "def rotation_consistency_loss(model, x, K=2, max_deg=15.0, temperature=1.0):\n",
    "    \"\"\"Fixed rotation consistency loss\"\"\"\n",
    "    with torch.no_grad():\n",
    "        base_logits, _, _ = model.forward(x)\n",
    "        base_logits = base_logits / temperature\n",
    "        p_base = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    kl_losses = []\n",
    "    for _ in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        logits_r, _, _ = model.forward(xr)\n",
    "        p_r = F.log_softmax(logits_r / temperature, dim=-1)\n",
    "        kl = F.kl_div(p_r, p_base, reduction='batchmean', log_target=False)\n",
    "        kl_losses.append(kl)\n",
    "\n",
    "    return sum(kl_losses) / len(kl_losses)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _entropy(p):\n",
    "    return -(p * (p.clamp_min(1e-8).log())).sum(dim=-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def roco_tta_predict(model, x, steps=3, K=4, lr=5e-3, max_deg=20.0, temperature=1.0, weight_cons=1.0):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    # Only adapt the adapter parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    for p in model.adapter.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    opt = torch.optim.Adam(model.adapter.parameters(), lr=lr)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad()\n",
    "        lo0, _, _ = model.forward(x)\n",
    "        p0 = F.softmax(lo0 / temperature, dim=-1)\n",
    "        ent = _entropy(p0).mean()\n",
    "\n",
    "        kl_losses = []\n",
    "        for _k in range(K):\n",
    "            xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "            lor, _, _ = model.forward(xr)\n",
    "            pr = F.log_softmax(lor / temperature, dim=-1)\n",
    "            kl = F.kl_div(pr, p0.detach(), reduction='batchmean', log_target=False)\n",
    "            kl_losses.append(kl)\n",
    "\n",
    "        kl_avg = sum(kl_losses) / len(kl_losses)\n",
    "        loss = ent + weight_cons * kl_avg\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # Ensemble prediction\n",
    "    probs = []\n",
    "    for _k in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        lor, _, _ = model.forward(xr)\n",
    "        probs.append(F.softmax(lor, dim=-1))\n",
    "    probs = torch.stack(probs, dim=0).mean(dim=0)\n",
    "\n",
    "    # Restore gradient requirements\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return probs\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2.7) Proto-Orbit Contrast (Fixed version)\n",
    "# ----------------------------\n",
    "def proto_orbit_loss(model, x, y, K=2, max_deg=20.0, temperature=0.2):\n",
    "    \"\"\"Fixed proto-orbit loss with proper gradient handling\"\"\"\n",
    "    proto = model.prototypes.detach()\n",
    "    proto_n = F.normalize(proto, dim=-1)\n",
    "\n",
    "    # Check if prototypes are initialized\n",
    "    if torch.all(proto == 0):\n",
    "        # Return zero loss but maintain gradient flow\n",
    "        dummy_loss = torch.zeros(1, device=x.device, requires_grad=True)\n",
    "        return dummy_loss.sum()\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        _, _, z_r = model(xr)\n",
    "        z_r = F.normalize(z_r, dim=-1)\n",
    "        logits = (z_r @ proto_n.t()) / temperature\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset & Dataloader (Fixed version)\n",
    "# ----------------------------\n",
    "class UCIHARInertial(Dataset):\n",
    "    def __init__(self, X, y, scaler: StandardScaler=None, train: bool=True,\n",
    "                 rot_ssl: bool=True, num_rotations: int=4, p_rotate_ssl: float=0.5):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.train = train\n",
    "        self.rot_ssl = rot_ssl\n",
    "        self.num_rot = num_rotations\n",
    "        self.p_rotate_ssl = p_rotate_ssl\n",
    "\n",
    "        # Standardization per-channel\n",
    "        self.scaler = scaler\n",
    "        if self.scaler is not None:\n",
    "            N, T, C = self.X.shape\n",
    "            X2 = self.X.reshape(-1, C)\n",
    "            X2 = self.scaler.transform(X2)\n",
    "            self.X = X2.reshape(N, T, C)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "        # Default values\n",
    "        rot_label = torch.tensor(0, dtype=torch.long)\n",
    "        x_ssl = x.clone()\n",
    "        apply_rot = False\n",
    "\n",
    "        if self.train and self.rot_ssl and np.random.rand() < self.p_rotate_ssl:\n",
    "            apply_rot = True\n",
    "            x_ssl_batch, rot_label_batch = make_rotation_class(x.unsqueeze(0), num_rotations=self.num_rot)\n",
    "            x_ssl = x_ssl_batch.squeeze(0)\n",
    "            rot_label = rot_label_batch.squeeze(0) if rot_label_batch.dim() > 0 else rot_label_batch\n",
    "\n",
    "        return x_ssl, y, rot_label, torch.tensor(apply_rot, dtype=torch.bool)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Training & Eval (Fixed version)\n",
    "# ----------------------------\n",
    "def train_one_epoch(model, loader, optimizer, device,\n",
    "                    lambda_rot=0.5, lambda_rc=0.0,\n",
    "                    lambda_orbit=0.2, orbit_K=2, orbit_tau=0.2, orbit_max_deg=20.0):\n",
    "    model.train()\n",
    "    total_loss = total_act = total_rot = total_orbit = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch_idx, (xb, yb, rb, msk) in enumerate(loader):\n",
    "        try:\n",
    "            xb, yb, rb, msk = xb.to(device), yb.to(device), rb.to(device), msk.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            lo_act, lo_rot, z = model(xb)\n",
    "            loss_act = F.cross_entropy(lo_act, yb)\n",
    "\n",
    "            # Rotation SSL loss (fixed gradient handling)\n",
    "            if msk.any():\n",
    "                # Only compute loss for rotated samples\n",
    "                loss_rot = F.cross_entropy(lo_rot[msk], rb[msk])\n",
    "            else:\n",
    "                # Create a zero loss that maintains gradient flow\n",
    "                loss_rot = torch.zeros_like(loss_act)\n",
    "\n",
    "            # Update prototypes (no gradients)\n",
    "            with torch.no_grad():\n",
    "                model.update_prototypes(z, yb)\n",
    "\n",
    "            # Proto-Orbit loss (fixed version)\n",
    "            if lambda_orbit > 0.0 and model.proto_counts.sum() > 0:\n",
    "                loss_orbit = proto_orbit_loss(model, xb, yb, K=orbit_K,\n",
    "                                            max_deg=orbit_max_deg, temperature=orbit_tau)\n",
    "            else:\n",
    "                loss_orbit = torch.zeros_like(loss_act)\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_act + lambda_rot * loss_rot + lambda_orbit * loss_orbit\n",
    "\n",
    "            # Optional rotation-consistency\n",
    "            if lambda_rc > 0:\n",
    "                rc = rotation_consistency_loss(model, xb, K=2, max_deg=15.0)\n",
    "                loss = loss + lambda_rc * rc\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            bs = xb.size(0)\n",
    "            n += bs\n",
    "            total_loss += float(loss.item()) * bs\n",
    "            total_act += float(loss_act.item()) * bs\n",
    "            total_rot += float(loss_rot.item()) * bs\n",
    "            total_orbit += float(loss_orbit.item()) * bs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return total_loss/n, total_act/n, total_rot/n, total_orbit/n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, use_tta=False):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for xb, yb, _, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        y_true.append(yb.numpy())\n",
    "\n",
    "        if use_tta:\n",
    "            probs = roco_tta_predict(model, xb, steps=2, K=3, lr=3e-3,\n",
    "                                   max_deg=20.0, weight_cons=1.0)\n",
    "            pred = probs.argmax(dim=-1).cpu().numpy()\n",
    "        else:\n",
    "            lo, _, _ = model(xb)\n",
    "            pred = lo.argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Main (Fixed version)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Use current working directory\n",
    "    root = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "    data_dir = download_and_extract_uci(root)\n",
    "\n",
    "    Xtr, ytr, Xte, yte, label_names = load_uci_inertial(data_dir)\n",
    "    print(\"[Data] Train:\", Xtr.shape, \" Test:\", Xte.shape)\n",
    "\n",
    "    # Standardize per-channel using train only\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Xtr.reshape(-1, Xtr.shape[2]))\n",
    "\n",
    "    train_ds = UCIHARInertial(Xtr, ytr, scaler=scaler, train=True,\n",
    "                              rot_ssl=True, num_rotations=4, p_rotate_ssl=0.7)\n",
    "    test_ds = UCIHARInertial(Xte, yte, scaler=scaler, train=False,\n",
    "                             rot_ssl=False, num_rotations=4, p_rotate_ssl=0.0)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Device] Using: {device}\")\n",
    "\n",
    "    model = InceptionMK(input_channels=9, stem_out=64, block_out=32,\n",
    "                        embedding_dim=128, num_classes=6, num_rotations=4).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-5)\n",
    "\n",
    "    best = {\"acc\": 0.0, \"f1\": 0.0, \"state\": None}\n",
    "    epochs = 25\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_act, tr_rot, tr_orb = train_one_epoch(\n",
    "            model, train_loader, optimizer, device,\n",
    "            lambda_rot=0.3,      # Reduced for stability\n",
    "            lambda_rc=0.0,       # Disabled for now\n",
    "            lambda_orbit=0.1,    # Reduced for stability\n",
    "            orbit_K=1,           # Reduced K\n",
    "            orbit_tau=0.3,\n",
    "            orbit_max_deg=15.0   # Reduced rotation\n",
    "        )\n",
    "\n",
    "        acc, f1 = evaluate(model, test_loader, device, use_tta=False)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {ep:02d}] loss={tr_loss:.4f} \"\n",
    "              f\"(act={tr_act:.4f}, rot={tr_rot:.4f}, orbit={tr_orb:.4f}) | \"\n",
    "              f\"test acc={acc*100:.2f}% f1={f1:.4f}\")\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"acc\": acc, \"f1\": f1, \"state\": {k: v.cpu() for k, v in model.state_dict().items()}})\n",
    "            print(f\"  ★ New best F1: {f1:.4f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if best[\"state\"] is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
    "\n",
    "    # Final evaluation\n",
    "    acc_off, f1_off = evaluate(model, test_loader, device, use_tta=False)\n",
    "    print(f\"\\n[Final - No TTA] ACC={acc_off*100:.2f}%  F1={f1_off:.4f}\")\n",
    "\n",
    "    try:\n",
    "        acc_tta, f1_tta = evaluate(model, test_loader, device, use_tta=True)\n",
    "        print(f\"[Final - With TTA] ACC={acc_tta*100:.2f}%  F1={f1_tta:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"TTA evaluation failed: {e}\")\n",
    "\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0fHnP2XISHV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "DzDwVxIS8bc3",
    "outputId": "bb76fd9f-67a8-4cd2-d44f-b89687111276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Train: (7352, 128, 9)  Test: (2947, 128, 9)\n",
      "[Epoch 01] loss=1.5389 (act=0.8742, rot=0.9402, orbit=0.9729) | test acc=86.05 f1=0.8587\n",
      "[Epoch 02] loss=0.6819 (act=0.3839, rot=0.3197, orbit=0.6905) | test acc=90.09 f1=0.9006\n",
      "[Epoch 03] loss=0.4835 (act=0.2698, rot=0.2346, orbit=0.4819) | test acc=91.18 f1=0.9107\n",
      "[Epoch 04] loss=0.4018 (act=0.2315, rot=0.1837, orbit=0.3924) | test acc=91.38 f1=0.9132\n",
      "[Epoch 05] loss=0.3639 (act=0.2152, rot=0.1548, orbit=0.3563) | test acc=90.46 f1=0.9040\n",
      "[Epoch 06] loss=0.3284 (act=0.1982, rot=0.1273, orbit=0.3330) | test acc=91.35 f1=0.9131\n",
      "[Epoch 07] loss=0.3279 (act=0.1941, rot=0.1385, orbit=0.3228) | test acc=91.14 f1=0.9112\n",
      "[Epoch 08] loss=0.3059 (act=0.1838, rot=0.1209, orbit=0.3084) | test acc=90.60 f1=0.9052\n",
      "[Epoch 09] loss=0.2907 (act=0.1778, rot=0.1043, orbit=0.3036) | test acc=91.25 f1=0.9121\n",
      "[Epoch 10] loss=0.2853 (act=0.1733, rot=0.1048, orbit=0.2983) | test acc=91.69 f1=0.9166\n",
      "[Epoch 11] loss=0.2710 (act=0.1671, rot=0.0912, orbit=0.2914) | test acc=92.09 f1=0.9206\n",
      "[Epoch 12] loss=0.2571 (act=0.1577, rot=0.0837, orbit=0.2876) | test acc=90.60 f1=0.9042\n",
      "[Epoch 13] loss=0.2637 (act=0.1597, rot=0.0948, orbit=0.2827) | test acc=90.84 f1=0.9078\n",
      "[Epoch 14] loss=0.2527 (act=0.1552, rot=0.0829, orbit=0.2799) | test acc=91.52 f1=0.9152\n",
      "[Epoch 15] loss=0.2457 (act=0.1490, rot=0.0845, orbit=0.2720) | test acc=91.18 f1=0.9120\n",
      "[Epoch 16] loss=0.2410 (act=0.1483, rot=0.0786, orbit=0.2670) | test acc=91.69 f1=0.9166\n",
      "[Epoch 17] loss=0.2331 (act=0.1459, rot=0.0687, orbit=0.2645) | test acc=93.18 f1=0.9314\n",
      "[Epoch 18] loss=0.2409 (act=0.1470, rot=0.0813, orbit=0.2662) | test acc=91.99 f1=0.9197\n",
      "[Epoch 19] loss=0.2398 (act=0.1485, rot=0.0784, orbit=0.2608) | test acc=92.70 f1=0.9270\n",
      "[Epoch 20] loss=0.2323 (act=0.1445, rot=0.0715, orbit=0.2602) | test acc=91.21 f1=0.9122\n",
      "[Epoch 21] loss=0.2335 (act=0.1421, rot=0.0808, orbit=0.2549) | test acc=90.33 f1=0.9040\n",
      "[Epoch 22] loss=0.2266 (act=0.1408, rot=0.0686, orbit=0.2575) | test acc=92.60 f1=0.9258\n",
      "[Epoch 23] loss=0.2298 (act=0.1447, rot=0.0669, orbit=0.2587) | test acc=91.82 f1=0.9180\n",
      "[Epoch 24] loss=0.2333 (act=0.1443, rot=0.0768, orbit=0.2527) | test acc=91.89 f1=0.9186\n",
      "[Epoch 25] loss=0.2367 (act=0.1423, rot=0.0878, orbit=0.2529) | test acc=91.52 f1=0.9142\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1958893819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1958893819.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# Final: TTA OFF vs ON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0macc_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     \u001b[0macc_tta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_tta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Final Results (UCI-HAR inertial) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1958893819.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader, device, use_tta)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_tta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroco_tta_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_deg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_cons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1958893819.py\u001b[0m in \u001b[0;36mroco_tta_predict\u001b[0;34m(model, x, steps, K, lr, max_deg, temperature, weight_cons)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ment\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_cons\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UCI-HAR (Inertial Signals, raw 9ch) Training & Evaluation\n",
    "# InceptionMK + Rotation SSL + (optional) Rotation-Consistency + RoCo-TTA\n",
    "# + Proto-Orbit Contrast (class-prototype-based rotation-invariant loss)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Utilities: download & load UCI-HAR (raw inertial)\n",
    "# ----------------------------\n",
    "UCI_URL = \"https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\"\n",
    "\n",
    "def download_and_extract_uci(root: str):\n",
    "    root = Path(root)\n",
    "    data_dir = root / \"UCI_HAR_Dataset\"\n",
    "    if data_dir.exists():\n",
    "        return data_dir\n",
    "\n",
    "    root.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = root / \"uci_har.zip\"\n",
    "    print(\"[UCI] Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(UCI_URL, zip_path)\n",
    "\n",
    "    print(\"[UCI] Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(root)\n",
    "\n",
    "    # The archive extracts a folder \"UCI HAR Dataset\"\n",
    "    # unify name to UCI_HAR_Dataset\n",
    "    original = root / \"UCI HAR Dataset\"\n",
    "    if original.exists():\n",
    "        original.rename(data_dir)\n",
    "    zip_path.unlink(missing_ok=True)\n",
    "    print(\"[UCI] Ready at:\", str(data_dir))\n",
    "    return data_dir\n",
    "\n",
    "def _load_signal_file(path: Path) -> np.ndarray:\n",
    "    # Each file: N lines, each line has 128 floats separated by space\n",
    "    arr = np.loadtxt(path, dtype=np.float32)\n",
    "    # shape: (N, 128)\n",
    "    return arr\n",
    "\n",
    "def load_uci_inertial(data_dir: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_train: (N_train, 128, 9), y_train: (N_train,)\n",
    "      X_test:  (N_test, 128, 9), y_test:  (N_test,)\n",
    "      label_names: list of 6 activities\n",
    "    \"\"\"\n",
    "    base = data_dir\n",
    "\n",
    "    # 9 inertial signals\n",
    "    train_inertial = base / \"train\" / \"Inertial Signals\"\n",
    "    test_inertial  = base / \"test\" / \"Inertial Signals\"\n",
    "\n",
    "    files = [\n",
    "        \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
    "        \"body_acc_x\",  \"body_acc_y\",  \"body_acc_z\",\n",
    "        \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\"\n",
    "    ]\n",
    "    # Build arrays by stacking channel-wise\n",
    "    Xtr_list = []\n",
    "    Xte_list = []\n",
    "    for name in files:\n",
    "        ftr = train_inertial / f\"{name}_train.txt\"\n",
    "        fte = test_inertial  / f\"{name}_test.txt\"\n",
    "        Xtr_list.append(_load_signal_file(ftr))  # (Ntr,128)\n",
    "        Xte_list.append(_load_signal_file(fte))  # (Nte,128)\n",
    "\n",
    "    # Stack -> (C, N, 128) then transpose\n",
    "    X_train = np.stack(Xtr_list, axis=2)  # (Ntr,128,9)\n",
    "    X_test  = np.stack(Xte_list, axis=2)  # (Nte,128,9)\n",
    "\n",
    "    # Labels: 1..6\n",
    "    y_train = np.loadtxt(base / \"train\" / \"y_train.txt\", dtype=np.int64).ravel() - 1\n",
    "    y_test  = np.loadtxt(base / \"test\" / \"y_test.txt\", dtype=np.int64).ravel() - 1\n",
    "\n",
    "    # Label names\n",
    "    label_names = [\"WALKING\",\"WALKING_UPSTAIRS\",\"WALKING_DOWNSTAIRS\",\"SITTING\",\"STANDING\",\"LAYING\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, label_names\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Rotation utilities (3-axis group-wise)\n",
    "# ----------------------------\n",
    "def _batched_rotation_matrix(angles):  # angles: (B, 3) in radians: rx, ry, rz\n",
    "    B = angles.shape[0]\n",
    "    rx, ry, rz = angles[:, 0], angles[:, 1], angles[:, 2]\n",
    "\n",
    "    cx, sx = torch.cos(rx), torch.sin(rx)\n",
    "    cy, sy = torch.cos(ry), torch.sin(ry)\n",
    "    cz, sz = torch.cos(rz), torch.sin(rz)\n",
    "\n",
    "    Rx = torch.zeros(B, 3, 3, device=angles.device)\n",
    "    Rx[:, 0, 0] = 1.\n",
    "    Rx[:, 1, 1] = cx; Rx[:, 1, 2] = -sx\n",
    "    Rx[:, 2, 1] = sx; Rx[:, 2, 2] = cx\n",
    "\n",
    "    Ry = torch.zeros(B, 3, 3, device=angles.device)\n",
    "    Ry[:, 0, 0] = cy;  Ry[:, 0, 2] = sy\n",
    "    Ry[:, 1, 1] = 1.\n",
    "    Ry[:, 2, 0] = -sy; Ry[:, 2, 2] = cy\n",
    "\n",
    "    Rz = torch.zeros(B, 3, 3, device=angles.device)\n",
    "    Rz[:, 0, 0] = cz; Rz[:, 0, 1] = -sz\n",
    "    Rz[:, 1, 0] = sz; Rz[:, 1, 1] = cz\n",
    "    Rz[:, 2, 2] = 1.\n",
    "\n",
    "    R = Rz @ Ry @ Rx\n",
    "    return R  # (B, 3, 3)\n",
    "\n",
    "def apply_random_rotation_xyz(x, max_deg=20.0, group_size=3):\n",
    "    \"\"\"\n",
    "    x: (B, T, C)\n",
    "    C must be multiple of 3. UCI inertial: 9ch -> OK.\n",
    "    \"\"\"\n",
    "    B, T, C = x.shape\n",
    "    assert C % group_size == 0\n",
    "    G = C // group_size\n",
    "\n",
    "    max_rad = max_deg * math.pi / 180.0\n",
    "    angles = (torch.rand(B, 3, device=x.device) * 2 - 1) * max_rad\n",
    "    R = _batched_rotation_matrix(angles)  # (B,3,3)\n",
    "\n",
    "    x_rot = x.clone()\n",
    "    for g in range(G):\n",
    "        seg = x[:, :, g*group_size:(g+1)*group_size]      # (B, T, 3)\n",
    "        seg2 = seg.reshape(B*T, 3)\n",
    "        Rb = R.repeat_interleave(T, dim=0)                # (B*T,3,3)\n",
    "        seg_rot = (Rb @ seg2.unsqueeze(-1)).squeeze(-1)   # (B*T,3)\n",
    "        x_rot[:, :, g*group_size:(g+1)*group_size] = seg_rot.view(B, T, 3)\n",
    "    return x_rot\n",
    "\n",
    "# (1) make_rotation_class: 라벨 스칼라로 통일\n",
    "def make_rotation_class(x, num_rotations=4):\n",
    "    # x: (B,T,C)\n",
    "    device = x.device\n",
    "    B, T, C = x.shape\n",
    "    angles_deg = torch.tensor([0., 90., 180., 270.], device=device)\n",
    "    labels = torch.randint(0, len(angles_deg), (B,), device=device)  # (B,)\n",
    "    rad = angles_deg[labels] * math.pi / 180.0\n",
    "\n",
    "    angles = torch.stack([torch.zeros_like(rad), torch.zeros_like(rad), rad], dim=1)  # (B,3)\n",
    "    R = _batched_rotation_matrix(angles)  # (B,3,3)\n",
    "\n",
    "    x_rot = x.clone()\n",
    "    G = C // 3\n",
    "    for g in range(G):\n",
    "        seg = x[:, :, g*3:(g+1)*3]           # (B,T,3)\n",
    "        seg2 = seg.reshape(B*T, 3)\n",
    "        Rb = R.repeat_interleave(T, dim=0)   # (B*T,3,3)\n",
    "        seg_rot = (Rb @ seg2.unsqueeze(-1)).squeeze(-1)\n",
    "        x_rot[:, :, g*3:(g+1)*3] = seg_rot.view(B, T, 3)\n",
    "\n",
    "    # ★ 라벨 모양 통일: 배치가 1이면 스칼라로\n",
    "    if B == 1:\n",
    "        labels = labels.squeeze(0)  # shape: ()\n",
    "    return x_rot, labels\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Model (InceptionMK + adapter + TTA + Prototypes)\n",
    "# ----------------------------\n",
    "class DSConv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                   padding=kernel_size // 2, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        )\n",
    "        self.branch2 = DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch3 = DSConv1D(in_channels, out_channels, kernel_size=3)\n",
    "        self.branch4 = DSConv1D(in_channels, out_channels, kernel_size=5)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([\n",
    "            self.branch1(x), self.branch2(x),\n",
    "            self.branch3(x), self.branch4(x)\n",
    "        ], dim=1)\n",
    "        return self.relu(out)\n",
    "\n",
    "class MultiKernelBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch1 = DSConv1D(in_channels, out_channels, kernel_size=1)\n",
    "        self.branch2 = DSConv1D(in_channels, out_channels, kernel_size=3)\n",
    "        self.branch3 = DSConv1D(in_channels, out_channels, kernel_size=5)\n",
    "        self.branch4 = DSConv1D(in_channels, out_channels, kernel_size=7)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([\n",
    "            self.branch1(x), self.branch2(x),\n",
    "            self.branch3(x), self.branch4(x)\n",
    "        ], dim=1)\n",
    "        return self.relu(out)\n",
    "\n",
    "class FeatureAdapter(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(1, dim))\n",
    "        self.beta  = nn.Parameter(torch.zeros(1, dim))\n",
    "    def forward(self, z):\n",
    "        return z * self.gamma + self.beta\n",
    "\n",
    "class InceptionMK(nn.Module):\n",
    "    def __init__(self, input_channels=9, stem_out=64, block_out=32,\n",
    "                 embedding_dim=128, num_classes=6, num_rotations=4):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, stem_out, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm1d(stem_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.inception = InceptionBlock(stem_out, block_out)\n",
    "        self.mk_block  = MultiKernelBlock(block_out * 4, block_out)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc  = nn.Linear(block_out * 4, embedding_dim)\n",
    "        self.adapter = FeatureAdapter(embedding_dim)\n",
    "\n",
    "        self.head_act = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.head_rot = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_rotations)\n",
    "        )\n",
    "\n",
    "        # --- Proto-Orbit: class prototypes (EMA) ---\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.register_buffer(\"prototypes\", torch.zeros(num_classes, embedding_dim))\n",
    "        self.proto_m = 0.99  # EMA momentum\n",
    "\n",
    "    def forward(self, x):  # x: (B,T,C)\n",
    "        x = x.transpose(1, 2)           # -> (B,C,T)\n",
    "        x = self.stem(x)\n",
    "        x = self.inception(x)\n",
    "        x = self.mk_block(x)\n",
    "        x = self.gap(x).squeeze(-1)     # (B, 4*block_out)\n",
    "        z = self.fc(x)                  # (B, D)\n",
    "        z = self.adapter(z)\n",
    "        logits_act = self.head_act(z)\n",
    "        logits_rot = self.head_rot(z)\n",
    "        return logits_act, logits_rot, z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "        lo, _, _ = self.forward(x)\n",
    "        return lo\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_prototypes(self, z, y):\n",
    "        \"\"\"\n",
    "        z: (B, D) embedding (before heads), y: (B,) class labels\n",
    "        EMA로 클래스 프로토타입 업데이트. 첫 업데이트는 직접 할당.\n",
    "        \"\"\"\n",
    "        if z.numel() == 0:\n",
    "            return\n",
    "        z_n = F.normalize(z, dim=-1)\n",
    "        classes = y.unique()\n",
    "        for c in classes:\n",
    "            mask = (y == c)\n",
    "            if mask.any():\n",
    "                m = z_n[mask].mean(dim=0)\n",
    "                p = self.prototypes[c]\n",
    "                if torch.all(p == 0):\n",
    "                    self.prototypes[c] = m\n",
    "                else:\n",
    "                    self.prototypes[c] = self.proto_m * p + (1.0 - self.proto_m) * m\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2.5) Rotation-consistency (optional)\n",
    "# ----------------------------\n",
    "def rotation_consistency_loss(model, x, K=2, max_deg=15.0, temperature=1.0):\n",
    "    # Keep model in its current (training) mode; just detach base distribution.\n",
    "    with torch.no_grad():\n",
    "        base_logits, _, _ = model.forward(x)\n",
    "        base_logits = base_logits / temperature\n",
    "        p_base = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    kl_sum = 0.0\n",
    "    for _ in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        logits_r, _, _ = model.forward(xr)\n",
    "        p_r = F.log_softmax(logits_r / temperature, dim=-1)\n",
    "        kl_sum = kl_sum + F.kl_div(p_r, p_base, reduction='batchmean', log_target=False)\n",
    "\n",
    "    return kl_sum / K\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _entropy(p):\n",
    "    return -(p * (p.clamp_min(1e-8).log())).sum(dim=-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def roco_tta_predict(model, x, steps=3, K=4, lr=5e-3, max_deg=20.0, temperature=1.0, weight_cons=1.0):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    for p in model.adapter.parameters():\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    opt = torch.optim.Adam(model.adapter.parameters(), lr=lr)\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        lo0, _, _ = model.forward(x)\n",
    "        p0 = F.softmax(lo0 / temperature, dim=-1)\n",
    "        ent = _entropy(p0).mean()\n",
    "\n",
    "        kl = 0.0\n",
    "        for _k in range(K):\n",
    "            xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "            lor, _, _ = model.forward(xr)\n",
    "            pr = F.log_softmax(lor / temperature, dim=-1)\n",
    "            kl += F.kl_div(pr, p0.detach(), reduction='batchmean', log_target=False)\n",
    "        kl = kl / K\n",
    "\n",
    "        loss = ent + weight_cons * kl\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    probs = []\n",
    "    for _k in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        lor, _, _ = model.forward(xr)\n",
    "        probs.append(F.softmax(lor, dim=-1))\n",
    "    probs = torch.stack(probs, dim=0).mean(dim=0)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(True)\n",
    "    if was_training:\n",
    "        model.train()\n",
    "    return probs\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2.7) Proto-Orbit Contrast (InfoNCE/CE with class prototypes)\n",
    "# ----------------------------\n",
    "\n",
    "def proto_orbit_loss(model, x, y, K=2, max_deg=20.0, temperature=0.2):\n",
    "    \"\"\"\n",
    "    동일 샘플의 여러 회전 오빗 임베딩 z_r를 '클래스 프로토타입'에 수축.\n",
    "    logits = z_r · proto^T / tau  →  CE(logits, y)\n",
    "    \"\"\"\n",
    "    proto = model.prototypes.detach()  # Detach prototypes (they're updated via EMA)\n",
    "    proto_n = F.normalize(proto, dim=-1)\n",
    "\n",
    "    loss_sum = torch.tensor(0.0, device=x.device)\n",
    "    for _ in range(K):\n",
    "        xr = apply_random_rotation_xyz(x, max_deg=max_deg)\n",
    "        _, _, z_r = model(xr)                 # (B, D)\n",
    "        z_r = F.normalize(z_r, dim=-1)        # (B, D)\n",
    "        logits = (z_r @ proto_n.t()) / temperature  # (B, C)\n",
    "        loss_sum = loss_sum + F.cross_entropy(logits, y)\n",
    "\n",
    "    return loss_sum / K\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Dataset & Dataloader\n",
    "# ----------------------------\n",
    "class UCIHARInertial(Dataset):\n",
    "    def __init__(self, X, y, scaler: StandardScaler=None, train: bool=True,\n",
    "                 rot_ssl: bool=True, num_rotations: int=4, p_rotate_ssl: float=0.5):\n",
    "        \"\"\"\n",
    "        X: (N, 128, 9), y: (N,)\n",
    "        scaler: StandardScaler fitted on train (per-channel)\n",
    "        rot_ssl: if True, returns rotation SSL targets\n",
    "        \"\"\"\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.train = train\n",
    "        self.rot_ssl = rot_ssl\n",
    "        self.num_rot = num_rotations\n",
    "        self.p_rotate_ssl = p_rotate_ssl\n",
    "\n",
    "        # standardization per-channel using train scaler\n",
    "        self.scaler = scaler\n",
    "        if self.scaler is not None:\n",
    "            N, T, C = self.X.shape\n",
    "            X2 = self.X.reshape(-1, C)  # (N*T, C)\n",
    "            X2 = self.scaler.transform(X2)\n",
    "            self.X = X2.reshape(N, T, C)\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])  # (128,9)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "        # Initialize with default values\n",
    "        rot_label = torch.tensor(0, dtype=torch.long)\n",
    "        x_ssl = x.clone()\n",
    "        apply_rot = 0\n",
    "\n",
    "        if self.train and self.rot_ssl and np.random.rand() < self.p_rotate_ssl:\n",
    "            apply_rot = 1\n",
    "            x_ssl_batch, rot_label_batch = make_rotation_class(x.unsqueeze(0), num_rotations=self.num_rot)\n",
    "            x_ssl = x_ssl_batch.squeeze(0)\n",
    "            if rot_label_batch.dim() > 0:\n",
    "                rot_label = rot_label_batch.squeeze(0).long()\n",
    "            else:\n",
    "                rot_label = rot_label_batch.long()\n",
    "\n",
    "        # Return mask as bool tensor\n",
    "        msk = torch.tensor(apply_rot, dtype=torch.bool)\n",
    "        return x_ssl, y, rot_label, msk\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Training & Eval\n",
    "# ----------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device,\n",
    "                    lambda_rot=0.5, lambda_rc=0.0,\n",
    "                    lambda_orbit=0.2, orbit_K=2, orbit_tau=0.2, orbit_max_deg=20.0):\n",
    "    model.train()\n",
    "    total_loss = total_act = total_rot = total_orbit = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, yb, rb, msk in loader:\n",
    "        xb, yb, rb, msk = xb.to(device), yb.to(device), rb.to(device), msk.to(device)\n",
    "\n",
    "        lo_act, lo_rot, z = model(xb)\n",
    "        loss_act = F.cross_entropy(lo_act, yb)\n",
    "\n",
    "        # --- Rotation SSL (회전 샘플이 없으면 그래프 연결된 0 사용)\n",
    "        if msk.any():\n",
    "            loss_rot = F.cross_entropy(lo_rot[msk], rb[msk])\n",
    "        else:\n",
    "            loss_rot = 0.0 * lo_act.sum()\n",
    "\n",
    "        # --- 프로토타입 EMA 업데이트 (그래프 외부)\n",
    "        with torch.no_grad():\n",
    "            model.update_prototypes(z, yb)\n",
    "\n",
    "        # --- Proto-Orbit (끄면 그래프 연결된 0)\n",
    "        if lambda_orbit > 0.0:\n",
    "            loss_orbit = proto_orbit_loss(model, xb, yb,\n",
    "                                          K=orbit_K, max_deg=orbit_max_deg, temperature=orbit_tau)\n",
    "        else:\n",
    "            loss_orbit = 0.0 * lo_act.sum()\n",
    "\n",
    "        loss = loss_act + lambda_rot * loss_rot + lambda_orbit * loss_orbit\n",
    "\n",
    "        # (선택) Rotation-consistency\n",
    "        if lambda_rc > 0.0:\n",
    "            rc = rotation_consistency_loss(model, xb, K=2, max_deg=15.0)\n",
    "            loss = loss + lambda_rc * rc\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        n += bs\n",
    "        total_loss  += float(loss.detach()) * bs\n",
    "        total_act   += float(loss_act.detach()) * bs\n",
    "        total_rot   += float(loss_rot.detach()) * bs\n",
    "        total_orbit += float(loss_orbit.detach()) * bs\n",
    "\n",
    "    return total_loss/n, total_act/n, total_rot/n, total_orbit/n\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, use_tta=False):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb, _, _ in loader:\n",
    "        xb = xb.to(device)\n",
    "        y_true.append(yb.numpy())\n",
    "        if use_tta:\n",
    "            probs = roco_tta_predict(model, xb, steps=2, K=3, lr=3e-3, max_deg=20.0, weight_cons=1.0)\n",
    "            pred = probs.argmax(dim=-1).cpu().numpy()\n",
    "        else:\n",
    "            lo, _, _ = model(xb)\n",
    "            pred = lo.argmax(dim=-1).cpu().numpy()\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # UPDATED PATH: Use current directory instead of Google Drive\n",
    "    root = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "    data_dir = download_and_extract_uci(root)\n",
    "\n",
    "    Xtr, ytr, Xte, yte, label_names = load_uci_inertial(data_dir)\n",
    "    print(\"[Data] Train:\", Xtr.shape, \" Test:\", Xte.shape)\n",
    "\n",
    "    # Standardize per-channel using train only\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Xtr.reshape(-1, Xtr.shape[2]))  # (N*T, C)\n",
    "\n",
    "    train_ds = UCIHARInertial(Xtr, ytr, scaler=scaler, train=True,\n",
    "                              rot_ssl=True, num_rotations=4, p_rotate_ssl=0.7)\n",
    "    test_ds  = UCIHARInertial(Xte, yte, scaler=scaler, train=False,\n",
    "                              rot_ssl=False, num_rotations=4, p_rotate_ssl=0.0)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=0, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model  = InceptionMK(input_channels=9, stem_out=64, block_out=32,\n",
    "                         embedding_dim=128, num_classes=6, num_rotations=4).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "    best = {\"acc\": 0.0, \"f1\": 0.0, \"state\": None}\n",
    "    epochs = 25\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_act, tr_rot, tr_orb = train_one_epoch(\n",
    "            model, train_loader, optimizer, device,\n",
    "            lambda_rot=0.5,      # rotation SSL loss weight\n",
    "            lambda_rc=0.0,       # rotation-consistency KL (off by default)\n",
    "            lambda_orbit=0.2,    # Proto-Orbit loss weight\n",
    "            orbit_K=2,           # # rotated views per sample\n",
    "            orbit_tau=0.2,       # InfoNCE temperature\n",
    "            orbit_max_deg=20.0   # rotation range for orbit\n",
    "        )\n",
    "        acc, f1 = evaluate(model, test_loader, device, use_tta=False)\n",
    "        print(f\"[Epoch {ep:02d}] loss={tr_loss:.4f} (act={tr_act:.4f}, rot={tr_rot:.4f}, orbit={tr_orb:.4f}) | \"\n",
    "              f\"test acc={acc*100:.2f} f1={f1:.4f}\")\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"acc\": acc, \"f1\": f1, \"state\": {k:v.cpu() for k,v in model.state_dict().items()}})\n",
    "\n",
    "    # Load best (optional)\n",
    "    if best[\"state\"] is not None:\n",
    "        model.load_state_dict({k:v.to(device) for k,v in best[\"state\"].items()})\n",
    "\n",
    "    # Final: TTA OFF vs ON\n",
    "    acc_off, f1_off = evaluate(model, test_loader, device, use_tta=False)\n",
    "    acc_tta, f1_tta = evaluate(model, test_loader, device, use_tta=True)\n",
    "\n",
    "    print(\"\\n=== Final Results (UCI-HAR inertial) ===\")\n",
    "    print(f\"Base  : ACC={acc_off*100:.2f}%  Macro-F1={f1_off:.4f}\")\n",
    "    print(f\"TTA ON: ACC={acc_tta*100:.2f}%  Macro-F1={f1_tta:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# Additional debugging and improvement suggestions:\n",
    "\n",
    "def debug_batch_info(loader, device, model=None):\n",
    "    \"\"\"Debug function to check batch composition\"\"\"\n",
    "    print(\"\\n=== Debugging Batch Information ===\")\n",
    "    for i, (xb, yb, rb, msk) in enumerate(loader):\n",
    "        if i >= 3:  # Only check first 3 batches\n",
    "            break\n",
    "        xb, yb, rb, msk = xb.to(device), yb.to(device), rb.to(device), msk.to(device)\n",
    "        print(f\"Batch {i}: shape={xb.shape}, rotated_samples={msk.sum().item()}/{len(msk)}\")\n",
    "        if model is not None and msk.any():\n",
    "            with torch.no_grad():\n",
    "                lo_act, lo_rot, z = model(xb)\n",
    "                print(f\"  -> logits shapes: act={lo_act.shape}, rot={lo_rot.shape}, z={z.shape}\")\n",
    "                if msk.any():\n",
    "                    print(f\"  -> rotated logits: {lo_rot[msk].shape}, labels: {rb[msk].shape}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# Training with better error handling\n",
    "def safe_train_one_epoch(model, loader, optimizer, device,\n",
    "                        lambda_rot=0.5, lambda_rc=0.0,\n",
    "                        lambda_orbit=0.2, orbit_K=2, orbit_tau=0.2, orbit_max_deg=20.0):\n",
    "    model.train()\n",
    "    total_loss = total_act = total_rot = total_orbit = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch_idx, (xb, yb, rb, msk) in enumerate(loader):\n",
    "        try:\n",
    "            xb, yb, rb, msk = xb.to(device), yb.to(device), rb.to(device), msk.to(device)\n",
    "\n",
    "            lo_act, lo_rot, z = model(xb)\n",
    "            loss_act = F.cross_entropy(lo_act, yb)\n",
    "\n",
    "            # Safer rotation SSL handling\n",
    "            if msk.any():\n",
    "                rotated_indices = msk.nonzero(as_tuple=True)[0]\n",
    "                if len(rotated_indices) > 0:\n",
    "                    loss_rot = F.cross_entropy(lo_rot[rotated_indices], rb[rotated_indices])\n",
    "                else:\n",
    "                    loss_rot = torch.zeros_like(loss_act)\n",
    "            else:\n",
    "                loss_rot = torch.zeros_like(loss_act)\n",
    "\n",
    "            # EMA update for class prototypes\n",
    "            with torch.no_grad():\n",
    "                model.update_prototypes(z, yb)\n",
    "\n",
    "            # Proto-Orbit loss\n",
    "            if lambda_orbit > 0.0:\n",
    "                try:\n",
    "                    loss_orbit = proto_orbit_loss(model, xb, yb,\n",
    "                                                K=orbit_K, max_deg=orbit_max_deg, temperature=orbit_tau)\n",
    "                except Exception as e:\n",
    "                    print(f\"Proto-orbit loss error in batch {batch_idx}: {e}\")\n",
    "                    loss_orbit = torch.zeros_like(loss_act)\n",
    "            else:\n",
    "                loss_orbit = torch.zeros_like(loss_act)\n",
    "\n",
    "            loss = loss_act + lambda_rot * loss_rot + lambda_orbit * loss_orbit\n",
    "\n",
    "            # Optional rotation-consistency KL\n",
    "            if lambda_rc > 0:\n",
    "                try:\n",
    "                    rc = rotation_consistency_loss(model, xb, K=2, max_deg=15.0)\n",
    "                    loss = loss + lambda_rc * rc\n",
    "                except Exception as e:\n",
    "                    print(f\"Rotation consistency loss error in batch {batch_idx}: {e}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            n += bs\n",
    "            total_loss += float(loss.detach()) * bs\n",
    "            total_act += float(loss_act.detach()) * bs\n",
    "            total_rot += float(loss_rot.detach()) * bs\n",
    "            total_orbit += float(loss_orbit.detach()) * bs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            print(f\"Batch shapes: xb={xb.shape if 'xb' in locals() else 'N/A'}, \"\n",
    "                  f\"yb={yb.shape if 'yb' in locals() else 'N/A'}, \"\n",
    "                  f\"rb={rb.shape if 'rb' in locals() else 'N/A'}, \"\n",
    "                  f\"msk={msk.shape if 'msk' in locals() else 'N/A'}\")\n",
    "            continue\n",
    "\n",
    "    return total_loss/n, total_act/n, total_rot/n, total_orbit/n\n",
    "\n",
    "\n",
    "# Alternative main function with better debugging\n",
    "def main_debug():\n",
    "    \"\"\"Main function with extensive debugging\"\"\"\n",
    "    print(\"Starting UCI-HAR training with debugging...\")\n",
    "\n",
    "    root = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "    data_dir = download_and_extract_uci(root)\n",
    "\n",
    "    Xtr, ytr, Xte, yte, label_names = load_uci_inertial(data_dir)\n",
    "    print(f\"[Data] Train: {Xtr.shape}, Test: {Xte.shape}\")\n",
    "    print(f\"[Data] Labels: {np.unique(ytr)} -> {label_names}\")\n",
    "\n",
    "    # Data standardization\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Xtr.reshape(-1, Xtr.shape[2]))\n",
    "\n",
    "    train_ds = UCIHARInertial(Xtr, ytr, scaler=scaler, train=True,\n",
    "                              rot_ssl=True, num_rotations=4, p_rotate_ssl=0.7)\n",
    "    test_ds = UCIHARInertial(Xte, yte, scaler=scaler, train=False,\n",
    "                             rot_ssl=False, num_rotations=4, p_rotate_ssl=0.0)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Device] Using: {device}\")\n",
    "\n",
    "    model = InceptionMK(input_channels=9, stem_out=64, block_out=32,\n",
    "                        embedding_dim=128, num_classes=6, num_rotations=4).to(device)\n",
    "\n",
    "    # Debug batch info\n",
    "    debug_batch_info(train_loader, device, model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-5)\n",
    "\n",
    "    best = {\"acc\": 0.0, \"f1\": 0.0, \"state\": None}\n",
    "    epochs = 20  # Reduced for debugging\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tr_loss, tr_act, tr_rot, tr_orb = safe_train_one_epoch(\n",
    "            model, train_loader, optimizer, device,\n",
    "            lambda_rot=0.3,      # Reduced weight\n",
    "            lambda_rc=0.0,       # Disabled for stability\n",
    "            lambda_orbit=0.1,    # Reduced weight\n",
    "            orbit_K=1,           # Reduced K for stability\n",
    "            orbit_tau=0.3,\n",
    "            orbit_max_deg=15.0   # Reduced rotation\n",
    "        )\n",
    "\n",
    "        acc, f1 = evaluate(model, test_loader, device, use_tta=False)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {ep:02d}] loss={tr_loss:.4f} \"\n",
    "              f\"(act={tr_act:.4f}, rot={tr_rot:.4f}, orbit={tr_orb:.4f}) | \"\n",
    "              f\"test acc={acc*100:.2f}% f1={f1:.4f} | lr={optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"acc\": acc, \"f1\": f1, \"state\": {k: v.cpu() for k, v in model.state_dict().items()}})\n",
    "            print(f\"  ★ New best F1: {f1:.4f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if best[\"state\"] is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best[\"state\"].items()})\n",
    "        print(f\"\\n[Best Model] Loaded with F1: {best['f1']:.4f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    acc_off, f1_off = evaluate(model, test_loader, device, use_tta=False)\n",
    "    print(f\"\\n[Final Eval - No TTA] ACC={acc_off*100:.2f}%  F1={f1_off:.4f}\")\n",
    "\n",
    "    # TTA evaluation (optional, might be slow)\n",
    "    try:\n",
    "        acc_tta, f1_tta = evaluate(model, test_loader, device, use_tta=True)\n",
    "        print(f\"[Final Eval - With TTA] ACC={acc_tta*100:.2f}%  F1={f1_tta:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"TTA evaluation failed: {e}\")\n",
    "\n",
    "    print(\"\\n=== Training Complete ===\")\n",
    "    return model, best\n",
    "\n",
    "\n",
    "# Uncomment to run the debug version instead:\n",
    "# main_debug()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
